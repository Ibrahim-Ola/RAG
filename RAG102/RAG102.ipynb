{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Load libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.rag101 import NaiveRAG\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "We have seen a naive implementation of RAG in RAG101. However, one of the problems we metioned was about our chatbot not having memory. Consider the conversations below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spring rain falls gently,\n",
      "Buds bloom, waking from winter's sleep,\n",
      "Nature reborn, anew.\n"
     ]
    }
   ],
   "source": [
    "rag = NaiveRAG(model=None, tokenizer=None) ## initialize RAG\n",
    "\n",
    "## question without context\n",
    "question = \"Write me a haiku about the weather.\"\n",
    "\n",
    "response=rag.qa(question=question) # first response takes a while because of model loading and initialization.\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, the text you provided is a haiku. A haiku is a form of traditional Japanese poetry that consists of three lines with a 5-7-5 syllable pattern. The first line has five syllables, the second line has seven, and the third line has five again. The text you gave \"Always assist... positive.\" follows this syllable pattern: 5-7-5. It also conveys a sense of purpose and positive message which is common in Haiku.\n"
     ]
    }
   ],
   "source": [
    "follow_up_question = \"Are you sure this is a haiku? Analyze it for me.\"\n",
    "\n",
    "response=rag.qa(question=follow_up_question) \n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I apologize if my previous response did not meet your expectations. I will make sure to always assist you with care, respect, and honesty. I will avoid any harmful, unethical, prejudiced, or negative content, and ensure that my responses promote fairness and positivity. I am here to provide the most accurate and helpful information possible. If you have a specific question or request, please let me know and I will do my best to assist you.\n",
      "\n",
      "Regarding your statement \"your analysis is wrong\", I would need more context in order to understand what you are referring to. My responses are generated automatically based on the input provided by the user, so there may be some misunderstanding or misinterpretation of your question or request. Please let me know if you would like me to clarify or correct any of my previous responses.\n"
     ]
    }
   ],
   "source": [
    "follow_up_question = \"No, your analysis is wrong.\"\n",
    "\n",
    "response=rag.qa(question=follow_up_question) \n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What did you Notice?\n",
    "\n",
    "Our model doesn't remember our previous conversations. By design, LLMs doesn't have memory. We must build memory into our system to enable a conversation. We can achieve this if our LLM has access to previous (or at least a window of previous) conversations. LangChain supports a number of memory algorithms. For RAG102, we will concentrate on `ConversationBufferMemory`.\n",
    "\n",
    "* [ConversationBufferMemory](https://python.langchain.com/docs/modules/memory/types/buffer): This is the simplest form of memory. It stores all conversations between the AI and the USER and passes them into the prompt template when the USER asks a new question. You can think of this history as `context` in our previous implementation (RAG101), but here, the context is the chat history. Now let's implement the `ConversationBufferMemory` in LangChain. We will start by modifying the mixtral instruction format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_with_context_to_mixtral_template(instruction: str, history: str, safety_mode: bool = True) -> str:\n",
    "\n",
    "    history_instruction = f\"The following is a conversation between a human and an AI assistant.\"\n",
    "\n",
    "    if safety_mode:\n",
    "        safety_prompt = (\n",
    "            \"The AI assistant always assist with care, respect, and truth. The AI assistant respond with utmost utility yet securely. \"\n",
    "            \"The AI assistant avoids harmful, unethical, prejudiced, or negative content and ensures replies promote fairness and positivity.\"\n",
    "        )\n",
    "\n",
    "        instruction = f\"{history_instruction} {safety_prompt}\\n\\nCurrent conversation: {history}\\n\\nHuman: {instruction}\"\n",
    "\n",
    "    else:\n",
    "        instruction = f\"{history_instruction}\\n\\nCurrent conversation: {history} \\n\\nHuman: {instruction}\"\n",
    "    \n",
    "    chat = [\n",
    "        {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n",
    "        {\"role\": \"user\", \"content\": instruction}\n",
    "    ]\n",
    "\n",
    "    return rag.tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s>[INST] The following is a conversation between a human and an AI assistant.\n",
      "\n",
      "Current conversation: {history} \n",
      "\n",
      "Human: {input} [/INST]\n"
     ]
    }
   ],
   "source": [
    "history=\"{history}\"\n",
    "question = \"{input}\"\n",
    "formatted_text=text_with_context_to_mixtral_template(instruction=question, history=history, safety_mode=False)\n",
    "print(formatted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat with Mixtral8x7B\n",
    "\n",
    "* Now that the instruction format is set up let's have some conversations. I'll turn on verbose so that you can see what's going on under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s>[INST] The following is a conversation between a human and an AI assistant. The AI assistant always assist with care, respect, and truth. The AI assistant respond with utmost utility yet securely. The AI assistant avoids harmful, unethical, prejudiced, or negative content and ensures replies promote fairness and positivity.\n",
      "\n",
      "Current conversation: \n",
      "\n",
      "Human: hi [/INST]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " AI Assistant: Hello! How can I assist you today? Is there something specific you would like to know or discuss? I'm here to help and provide accurate information in a respectful and positive manner.\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferMemory(memory_key=\"history\") # let's define the memory\n",
    "\n",
    "input_text=\"{input}\"\n",
    "chat_history=\"{history}\"\n",
    "template = text_with_context_to_mixtral_template(instruction=input_text, history=history) # mixtral's instruction format\n",
    "\n",
    "# let's define the prompt using LangChain's PromptTemplate\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# let's link componenet with LLMChain\n",
    "conversation = LLMChain(\n",
    "    llm=rag.model,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "## let's chat\n",
    "response=conversation({\"input\": \"hi\"})\n",
    "print(response['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s>[INST] The following is a conversation between a human and an AI assistant. The AI assistant always assist with care, respect, and truth. The AI assistant respond with utmost utility yet securely. The AI assistant avoids harmful, unethical, prejudiced, or negative content and ensures replies promote fairness and positivity.\n",
      "\n",
      "Current conversation: Human: hi\n",
      "AI:  AI Assistant: Hello! How can I assist you today? Is there something specific you would like to know or discuss? I'm here to help and provide accurate information in a respectful and positive manner.\n",
      "\n",
      "Human: Write me a haiku about USA. [/INST]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Of course! Here's a haiku for you:\n",
      "\n",
      "Endless rolling plains,\n",
      "Red, white, and blue ablaze,\n",
      "Unity in vastness.\n",
      "\n",
      "Would you like to learn more about the United States or any other topic? I'm here to help.\n"
     ]
    }
   ],
   "source": [
    "response=conversation({\"input\": \"Write me a haiku about USA.\"})\n",
    "print(response['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s>[INST] The following is a conversation between a human and an AI assistant. The AI assistant always assist with care, respect, and truth. The AI assistant respond with utmost utility yet securely. The AI assistant avoids harmful, unethical, prejudiced, or negative content and ensures replies promote fairness and positivity.\n",
      "\n",
      "Current conversation: Human: hi\n",
      "AI:  AI Assistant: Hello! How can I assist you today? Is there something specific you would like to know or discuss? I'm here to help and provide accurate information in a respectful and positive manner.\n",
      "Human: Write me a haiku about USA.\n",
      "AI:  Of course! Here's a haiku for you:\n",
      "\n",
      "Endless rolling plains,\n",
      "Red, white, and blue ablaze,\n",
      "Unity in vastness.\n",
      "\n",
      "Would you like to learn more about the United States or any other topic? I'm here to help.\n",
      "\n",
      "Human: Thank you! Analyze it for me. [/INST]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Sure! A haiku is a form of traditional Japanese poetry that consists of three lines with a 5-7-5 syllable pattern. The first line has five syllables, the second line has seven, and the third line has five again.\n",
      "\n",
      "My haiku about the USA focuses on some of the country's natural features (the \"endless rolling plains\") as well as its symbolic colors (red, white, and blue). The final line, \"Unity in vastness,\" highlights the way that the United States is made up of many different states and regions, but is bound together by a shared sense of national identity. This idea of unity despite diversity is an important aspect of American culture and history.\n",
      "\n",
      "I hope this analysis helps to clarify the meaning behind the haiku. Let me know if you have any other questions or if there is anything else I can help you with.\n"
     ]
    }
   ],
   "source": [
    "response=conversation({\"input\": \"Thank you! Analyze it for me.\"})\n",
    "print(response['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can see how this conversation unfolds. For a chatbot application, we would set `verbose=False`. Other memory algorithm supported by LangChain include:\n",
    "\n",
    "1. [ConversationBufferWindowMemory](https://python.langchain.com/docs/modules/memory/types/buffer_window).\n",
    "2. [ConversationEntityMemory](https://python.langchain.com/docs/modules/memory/types/entity_summary_memory).\n",
    "3. [ConversationKGMemory](https://python.langchain.com/docs/modules/memory/types/kg).\n",
    "4. [ConversationSummaryMemory](https://python.langchain.com/docs/modules/memory/types/summary).\n",
    "5. [ConversationSummaryBufferMemory](https://python.langchain.com/docs/modules/memory/types/summary_buffer).\n",
    "6. [ConversationTokenBufferMemory](https://python.langchain.com/docs/modules/memory/types/token_buffer).\n",
    "7. [VectorStoreRetrieverMemory](https://python.langchain.com/docs/modules/memory/types/vectorstore_retriever_memory)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it Together\n",
    "\n",
    "We will create a new script that puts all the code together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatBot(NaiveRAG):\n",
    "\n",
    "    def __init__(self, model=None, tokenizer=None, conversation_chain=None, model_template=None, safety_mode: bool = True):\n",
    "        super().__init__(model, tokenizer)\n",
    "\n",
    "        self.safety_mode = safety_mode\n",
    "        if model_template is None:\n",
    "            self.model_template = self.text_with_context_to_mixtral_template(instruction=\"{input}\", history=\"{history}\", safety_mode=self.safety_mode)\n",
    "        else:\n",
    "            self.model_template = model_template\n",
    "        \n",
    "        if conversation_chain is None:\n",
    "            self.conversation_chain = self.create_conversation()\n",
    "        else:\n",
    "            self.conversation_chain = conversation_chain\n",
    "\n",
    "    def text_with_context_to_mixtral_template(self, instruction: str, history: str, safety_mode: bool = True) -> str:\n",
    "\n",
    "        history_instruction = f\"The following is a conversation between a human and an AI assistant.\"\n",
    "\n",
    "        if safety_mode:\n",
    "            safety_prompt = (\n",
    "                \"The AI assistant always assist with care, respect, and truth. The AI assistant respond with utmost utility yet securely. \"\n",
    "                \"The AI assistant avoids harmful, unethical, prejudiced, or negative content and ensures replies promote fairness and positivity.\"\n",
    "            )\n",
    "\n",
    "            instruction = f\"{history_instruction} {safety_prompt}\\n\\nCurrent conversation: {history}\\n\\nHuman: {instruction}\"\n",
    "\n",
    "        else:\n",
    "            instruction = f\"{history_instruction}\\n\\nCurrent conversation: {history} \\n\\nHuman: {instruction}\"\n",
    "        \n",
    "        chat = [\n",
    "            {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n",
    "            {\"role\": \"user\", \"content\": instruction}\n",
    "        ]\n",
    "\n",
    "        return self.tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=False)\n",
    "    \n",
    "    def create_conversation(self) -> str:\n",
    "\n",
    "        memory = ConversationBufferMemory(memory_key=\"history\") # let's define the memory\n",
    "\n",
    "        # let's define the prompt using LangChain's PromptTemplate\n",
    "        prompt_template = PromptTemplate.from_template(self.model_template)\n",
    "\n",
    "        # let's link componenet with LLMChain\n",
    "        conversation = LLMChain(\n",
    "            llm=self.model,\n",
    "            prompt=prompt_template,\n",
    "            verbose=True,\n",
    "            memory=memory\n",
    "        )\n",
    "        return conversation\n",
    "    \n",
    "    def qa(self, question: str) -> str:\n",
    "\n",
    "        response = self.conversation_chain({\"input\": question})\n",
    "        return response['text'].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot=ChatBot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
