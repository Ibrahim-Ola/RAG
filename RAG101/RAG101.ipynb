{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Load libraries:\n",
    "\n",
    "Note: See Appendix on how to get your Hugging Face ðŸ¤— API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import textwrap\n",
    "import transformers\n",
    "from dotenv import load_dotenv\n",
    "from langchain import HuggingFacePipeline, PromptTemplate, LLMChain\n",
    "\n",
    "load_dotenv() # take environment variables from .env.\n",
    "api_key = os.getenv(\"Huggingface_API_key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check available device\n",
    "\n",
    "Here, You are checking the device (`cuda`, `mps`, or `cpu`) available on your system. For Mac users, you will get either `cpu` or `mps`. For Windows or Linux users, you will get either `cpu` or `cuda`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Model\n",
    "\n",
    "Here, we would configure our and download the Mixtral-8x7B (M8x7B) model using Hugging Face's transformers. Setting `device_map=\"auto\"` first utilize the GPU(s) memory, then CPU memory if needed, and finally stores data on the disk when both memory types are full. Also, we are loading the 4-bits precision model to save memory.\n",
    "\n",
    "Link to M8x7B on ðŸ¤—: [Mixtral-8x7B](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4a6a5a1f04d4b43bcde067067ffe093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\" # the model id on ðŸ¤—\n",
    "\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    token=api_key\n",
    ")\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    device_map='auto',\n",
    "    token=api_key,\n",
    "    load_in_4bit=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model set to evaluation mode.\n"
     ]
    }
   ],
   "source": [
    "# set model to evaluation mode\n",
    "\n",
    "model.eval()\n",
    "\n",
    "print(\"Model set to evaluation mode.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer\n",
    "\n",
    "We will instantiate a `tokenizer` designed to process natural language input by converting it into token lists compatible with the input layer of the M8x7B LLM. Note that we set `padding_side='left'` because we are working with a *decoder only* model. You can learn mode about decoder only models here on [Hugging Face](https://huggingface.co/learn/nlp-course/chapter1/6?fw=pt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    token=api_key,\n",
    "    padding_side='left',\n",
    "    truncation_side='right'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction Format\n",
    "\n",
    "To get the most out of M8x7B, you must follow the instruction format as outlined by [MistralAi](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1#instruction-format). The instruction format for M8x7B is:\n",
    "\n",
    "```{bash}\n",
    "<s> [INST] Instruction [/INST] Model answer</s> [INST] Follow-up instruction [/INST]\n",
    "```\n",
    "\n",
    "Where:\n",
    "\n",
    "* `<s> `and `</s>` are special tokens used by MixtralAI to signify the beginning of string (BOS) and end of string (EOS).\n",
    "* Instruction is the user message.\n",
    "* Model answer is where the model response goes.\n",
    "* [/INST] and [INST] indicates the start and end of user messages.\n",
    "\n",
    "Note: for enforcing guardrails, prepend the instruction with your safety/syatem prompt. For this project, I will use the safety prompt used in the [Mistral 7B paper](https://arxiv.org/pdf/2310.06825.pdf).\n",
    "\n",
    "You can easily get the instruction form using ðŸ¤—'s `apply_chat_template()` method. Let's see an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Hello! [/INST]Hello. How can I help you today?</s>\n"
     ]
    }
   ],
   "source": [
    "# ðŸ¤—'s Approach\n",
    "\n",
    "chat = [\n",
    "  {\"role\": \"user\", \"content\": \"Hello!\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"Hello. How can I help you today?\"}\n",
    "]\n",
    "\n",
    "print(tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's create a custom function that takes the user's prompt and dynamically converts it to M8x7B's format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_mixtral_template(instruction: str, safety_mode: bool = True) -> str:\n",
    "\n",
    "    if safety_mode:\n",
    "        safety_prompt = (\n",
    "            \"Always assist with care, respect, and truth. Respond with utmost utility yet securely. \"\n",
    "            \"Avoid harmful,unethical, prejudiced, or negative content. Ensure replies promote fairness and positivity.\"\n",
    "        )\n",
    "        \n",
    "        instruction = f\"{safety_prompt} {instruction}\"\n",
    "    \n",
    "    chat = [\n",
    "        {\"role\": \"user\", \"content\": instruction},\n",
    "        {\"role\": \"assistant\", \"content\": \"Assistant: \"}\n",
    "    ]\n",
    "\n",
    "    return tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's define a function that formats our text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Always assist with care, respect, and\n",
      "truth. Respond with utmost utility yet securely.\n",
      "Avoid harmful,unethical, prejudiced, or negative\n",
      "content. Ensure replies promote fairness and\n",
      "positivity. Hello, I would like to book a flight\n",
      "to Paris. [/INST]Assistant: </s>\n"
     ]
    }
   ],
   "source": [
    "def text_wrap(text: str,) -> str:\n",
    "    filled_text = textwrap.fill(text, width=50)\n",
    "    print(filled_text)\n",
    "\n",
    "formatted_text=text_to_mixtral_template(\"Hello, I would like to book a flight to Paris.\", safety_mode=True)\n",
    "text_wrap(formatted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Text Generation Pipeline\n",
    "\n",
    "Let's create a text generation pipeline to plug into the LangChain API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_pipeline = transformers.pipeline(\n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,  \n",
    "    task='text-generation',\n",
    "    framework=\"pt\",\n",
    "    temperature=0.7,\n",
    "    max_new_tokens=512,  \n",
    "    repetition_penalty=1.1, # \n",
    "    do_sample=True, \n",
    ")\n",
    "\n",
    "local_llm =HuggingFacePipeline(\n",
    "    pipeline=hf_pipeline,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text=\"{question}\"\n",
    "question: str = \"What is the name of the current president of Nigeria?\"\n",
    "template = text_to_mixtral_template(input_text)\n",
    "\n",
    "prompt_template = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "# llm_chain = LLMChain(prompt=prompt_template, llm=local_llm)\n",
    "# result=llm_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['question'], template=\"<s> [INST] You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. \\nIf you don't know the answer to a question, please don't share false information. [/INST] {question} </s>\")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    }
   ],
   "source": [
    "input_text=\"{question}\"\n",
    "question: str = \"What is Twitter's new name?\"\n",
    "template = text_to_mixtral_template(input_text)\n",
    "\n",
    "prompt_template = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt_template, llm=local_llm)\n",
    "result=llm_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of now, Twitter has not announced any changes to their name. They are still known as Twitter, Inc. If there are any updates in the future, I will make sure to provide you with accurate and reliable information. Is there anything else I can assist you with?\n"
     ]
    }
   ],
   "source": [
    "print_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_result(result):\n",
    "    formatted_result = result['text'][result['text'].find(\"\\n\"):].strip()\n",
    "    print(formatted_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of my last update, the current president of Nigeria is Muhammadu Buhari. He has been in office since May 29, 2015. However, I recommend checking the most recent sources to confirm as this information might have changed.\n"
     ]
    }
   ],
   "source": [
    "print(result['text'][result['text'].find(\"\\n\"):].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template=PromptTemplate(\n",
    "    input_variables=[\"question\", \"context\"],\n",
    "    template=\"\"\"<s> [INST] You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. \n",
    "    If you don't know the answer to a question, please don't share false information. [/INST] Answer the following question using the provided context. \n",
    "    \\n context: {context} \\n\\n question: {question} </s>\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "context=\"\"\"\n",
    "On 29 May 1999, General Abdulsalami Abubakar stepped down, and handed over power to a former military head of state, Olusegun Obasanjo, after being elected some months prior. Obasanjo served two terms in office.\n",
    "\n",
    "On 29 May 2007, Umaru Musa Yar'Adua was sworn in as president of the Federal Republic of Nigeria and the 13th head of state completing the first successful transition of power, from one democratically elected president to another in Nigeria. Yar'Adua died on 5 May 2010 at the presidential villa, in Abuja, Nigeria, becoming the second head of state to die there after General Sani Abacha.\n",
    "\n",
    "On 6 May 2010, Vice President Goodluck Jonathan was sworn in as president of the Federal Republic of Nigeria and the 14th head of state.\n",
    "\n",
    "On 29 May 2015, Muhammadu Buhari, a former military head of state was sworn in as president of the Federal Republic of Nigeria and the 15th head of state after winning the general election. He also served two terms in office.\n",
    "\n",
    "On 29 May 2023, Bola Tinubu was sworn in as president of the Federal Republic of Nigeria and the 16th head of state after winning the 2023 Nigerian general election.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    }
   ],
   "source": [
    "llm_chain = LLMChain(prompt=prompt_template, llm=local_llm)\n",
    "result=llm_chain.invoke({\"question\": question, \"context\": context})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current president of Nigeria is Bola Tinubu, who was sworn in as president on 29 May 2023.\n"
     ]
    }
   ],
   "source": [
    "print_result(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoskln",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
