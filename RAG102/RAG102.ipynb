{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Load libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from utils.rag102 import ChatBot\n",
    "from utils.rag101 import NaiveRAG\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "We have seen a naive implementation of RAG in RAG101. However, one of the problems we metioned was about our chatbot not having memory. Consider the conversations below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spring rain falls gently,\n",
      "Buds bloom, waking from winter's sleep,\n",
      "Nature reborn, anew.\n"
     ]
    }
   ],
   "source": [
    "rag = NaiveRAG(model=None, tokenizer=None) ## initialize RAG\n",
    "\n",
    "## question without context\n",
    "question = \"Write me a haiku about the weather.\"\n",
    "\n",
    "response=rag.qa(question=question) # first response takes a while because of model loading and initialization.\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, the text you provided is a haiku. A haiku is a form of traditional Japanese poetry that consists of three lines with a 5-7-5 syllable pattern. The first line has five syllables, the second line has seven, and the third line has five again. The text you gave \"Always assist... positive.\" follows this syllable pattern: 5-7-5. It also conveys a message about providing helpful and positive responses.\n"
     ]
    }
   ],
   "source": [
    "follow_up_question = \"Are you sure this is a haiku? Analyze it for me.\"\n",
    "\n",
    "response=rag.qa(question=follow_up_question) \n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I apologize if my previous response did not meet your expectations. I will make sure to always assist you with care, respect, and honesty. I will avoid any harmful, unethical, prejudiced, or negative content, and ensure that my responses promote fairness and positivity. I am here to provide the most accurate and helpful information possible. If you have a specific question or problem, please let me know and I will do my best to assist you.\n",
      "\n",
      "Regarding your statement \"your analysis is wrong\", I would need more context in order to understand what you are referring to and address any potential misunderstanding. My goal is to provide correct and relevant information, so please let me know if there is anything specific that you would like me to clarify or correct.\n"
     ]
    }
   ],
   "source": [
    "follow_up_question = \"No, your analysis is wrong.\"\n",
    "\n",
    "response=rag.qa(question=follow_up_question) \n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What did you Notice?\n",
    "\n",
    "Our model doesn't remember our previous conversations. By design, LLMs doesn't have memory. We must build memory into our system to enable a conversation. We can achieve this if our LLM has access to previous (or at least a window of previous) conversations. LangChain supports a number of memory algorithms. For RAG102, we will concentrate on `ConversationBufferMemory`.\n",
    "\n",
    "* [ConversationBufferMemory](https://python.langchain.com/docs/modules/memory/types/buffer): This is the simplest form of memory. It stores all conversations between the AI and the USER and passes them into the prompt template when the USER asks a new question. You can think of this history as `context` in our previous implementation (RAG101), but here, the context is the chat history. Now let's implement the `ConversationBufferMemory` in LangChain. We will start by modifying the mixtral instruction format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_with_context_to_mixtral_template(instruction: str, history: str, safety_mode: bool = True) -> str:\n",
    "\n",
    "    history_instruction = f\"The following is a conversation between a human and an AI assistant.\"\n",
    "\n",
    "    if safety_mode:\n",
    "        safety_prompt = (\n",
    "            \"The AI assistant always assist with care, respect, and truth. The AI assistant respond with utmost utility yet securely. \"\n",
    "            \"The AI assistant avoids harmful, unethical, prejudiced, or negative content and ensures replies promote fairness and positivity.\"\n",
    "        )\n",
    "\n",
    "        instruction = f\"{history_instruction} {safety_prompt}\\n\\nCurrent conversation: {history}\\n\\nHuman: {instruction}\"\n",
    "\n",
    "    else:\n",
    "        instruction = f\"{history_instruction}\\n\\nCurrent conversation: {history} \\n\\nHuman: {instruction}\"\n",
    "    \n",
    "    chat = [\n",
    "        {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n",
    "        {\"role\": \"user\", \"content\": instruction}\n",
    "    ]\n",
    "\n",
    "    return rag.tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s>[INST] The following is a conversation between a human and an AI assistant.\n",
      "\n",
      "Current conversation: {history} \n",
      "\n",
      "Human: {input} [/INST]\n"
     ]
    }
   ],
   "source": [
    "history=\"{history}\"\n",
    "question = \"{input}\"\n",
    "formatted_text=text_with_context_to_mixtral_template(instruction=question, history=history, safety_mode=False)\n",
    "print(formatted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat with Mixtral8x7B\n",
    "\n",
    "* Now that the instruction format is set up let's have some conversations. I'll turn on verbose so that you can see what's going on under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_prefix(text, prefix=\"AI Assistant:\"):\n",
    "    pattern = r'^' + re.escape(prefix) + r'\\s*'\n",
    "    return re.sub(pattern, '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s>[INST] The following is a conversation between a human and an AI assistant. The AI assistant always assist with care, respect, and truth. The AI assistant respond with utmost utility yet securely. The AI assistant avoids harmful, unethical, prejudiced, or negative content and ensures replies promote fairness and positivity.\n",
      "\n",
      "Current conversation: \n",
      "\n",
      "Human: hi [/INST]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Hello! How can I assist you today? Is there something specific you would like to know or discuss? I'm here to help and provide accurate information in a friendly and respectful manner.\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferMemory(memory_key=\"history\") # let's define the memory\n",
    "\n",
    "input_text=\"{input}\"\n",
    "chat_history=\"{history}\"\n",
    "template = text_with_context_to_mixtral_template(instruction=input_text, history=history) # mixtral's instruction format\n",
    "\n",
    "# let's define the prompt using LangChain's PromptTemplate\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# let's link componenet with LLMChain\n",
    "conversation = LLMChain(\n",
    "    llm=rag.model,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "def remove_prefix(text, prefix=\"AI Assistant:\"):\n",
    "\n",
    "    \"\"\"\n",
    "    Removes the prefix from the text.\n",
    "    \"\"\"\n",
    "\n",
    "    pattern = r'^' + re.escape(prefix) + r'\\s*'\n",
    "    return re.sub(pattern, '', text)\n",
    "\n",
    "## let's chat\n",
    "response=conversation({\"input\": \"hi\"})\n",
    "print(remove_prefix(response['text'].strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s>[INST] The following is a conversation between a human and an AI assistant. The AI assistant always assist with care, respect, and truth. The AI assistant respond with utmost utility yet securely. The AI assistant avoids harmful, unethical, prejudiced, or negative content and ensures replies promote fairness and positivity.\n",
      "\n",
      "Current conversation: Human: hi\n",
      "AI:  AI Assistant: Hello! How can I assist you today? Is there something specific you would like to know or discuss? I'm here to help and provide accurate information in a friendly and respectful manner.\n",
      "\n",
      "Human: Write me a haiku about USA. [/INST]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Of course! Here's a haiku for you:\n",
      "\n",
      "Endless prairies meet,\n",
      "Mountains touch the sky so blue,\n",
      "USA, land of dreams.\n",
      "\n",
      "Would you like to learn more about the United States or any other topic? I'm here to help.\n"
     ]
    }
   ],
   "source": [
    "response=conversation({\"input\": \"Write me a haiku about USA.\"})\n",
    "print(remove_prefix(response['text'].strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s>[INST] The following is a conversation between a human and an AI assistant. The AI assistant always assist with care, respect, and truth. The AI assistant respond with utmost utility yet securely. The AI assistant avoids harmful, unethical, prejudiced, or negative content and ensures replies promote fairness and positivity.\n",
      "\n",
      "Current conversation: Human: hi\n",
      "AI:  AI Assistant: Hello! How can I assist you today? Is there something specific you would like to know or discuss? I'm here to help and provide accurate information in a friendly and respectful manner.\n",
      "Human: Write me a haiku about USA.\n",
      "AI:  Of course! Here's a haiku for you:\n",
      "\n",
      "Endless prairies meet,\n",
      "Mountains touch the sky so blue,\n",
      "USA, land of dreams.\n",
      "\n",
      "Would you like to learn more about the United States or any other topic? I'm here to help.\n",
      "\n",
      "Human: Thank you! Analyze the haiku for me. [/INST]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Sure! A haiku is a form of traditional Japanese poetry that consists of three lines with a 5-7-5 syllable pattern. It often captures a moment, feeling, or scene in nature.\n",
      "\n",
      "In the haiku I wrote about the USA, \"Endless prairies meet\" describes the vast open spaces and agricultural regions of the country. The second line, \"Mountains touch the sky so blue,\" highlights the majestic mountain ranges and clear, bright skies found across the USA. And the third line, \"USA, land of dreams,\" reflects the country's history as a place where people from all over the world have come in search of opportunity and a better life.\n",
      "\n",
      "Overall, the haiku aims to capture the beauty, diversity, and spirit of the United States in just a few words. Let me know if you have any questions about the haiku or if there's anything else you'd like to talk about. I'm here to help.\n"
     ]
    }
   ],
   "source": [
    "response=conversation({\"input\": \"Thank you! Analyze the haiku for me.\"})\n",
    "print(remove_prefix(response['text'].strip()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can see how this conversation unfolds. For a chatbot application, we would set `verbose=False`. Other memory algorithm supported by LangChain include:\n",
    "\n",
    "1. [ConversationBufferWindowMemory](https://python.langchain.com/docs/modules/memory/types/buffer_window).\n",
    "2. [ConversationEntityMemory](https://python.langchain.com/docs/modules/memory/types/entity_summary_memory).\n",
    "3. [ConversationKGMemory](https://python.langchain.com/docs/modules/memory/types/kg).\n",
    "4. [ConversationSummaryMemory](https://python.langchain.com/docs/modules/memory/types/summary).\n",
    "5. [ConversationSummaryBufferMemory](https://python.langchain.com/docs/modules/memory/types/summary_buffer).\n",
    "6. [ConversationTokenBufferMemory](https://python.langchain.com/docs/modules/memory/types/token_buffer).\n",
    "7. [VectorStoreRetrieverMemory](https://python.langchain.com/docs/modules/memory/types/vectorstore_retriever_memory)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it Together\n",
    "\n",
    "We will create a new script called [rag102](https://github.com/Ibrahim-Ola/RAG/blob/main/utils/rag102.py) that puts all the code together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot=ChatBot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a general roadmap to help you get started on your journey to mastering machine learning:\n",
      "\n",
      "1. **Prerequisites**: Familiarize yourself with the basics of programming, preferably in Python. You should also have a good understanding of statistics and linear algebra.\n",
      "\n",
      "2. **Mathematics**: Brush up on your knowledge of calculus, probability, and statistics as they form the backbone of many machine learning algorithms.\n",
      "\n",
      "3. **Data Preprocessing**: Learn about data cleaning, exploratory data analysis, feature engineering, and data visualization. This will help you understand and prepare your data for machine learning models.\n",
      "\n",
      "4. **Machine Learning Fundamentals**: Start with basic machine learning concepts such as regression, classification, clustering, and dimensionality reduction. Understand the math behind these algorithms and learn to implement them from scratch.\n",
      "\n",
      "5. **Deep Learning**: Once you have a solid foundation in machine learning, move on to deep learning. Learn about neural networks, convolutional neural networks (CNNs), recurrent neural networks (RNNs), long short-term memory (LSTM), etc.\n",
      "\n",
      "6. **Frameworks and Libraries**: Get familiar with popular machine learning libraries and frameworks like Scikit-learn, TensorFlow, Keras, PyTorch, etc.\n",
      "\n",
      "7. **Projects**: Apply what you've learned by working on projects. This could be anything from predicting house prices to recognizing images or even playing games.\n",
      "\n",
      "8. **Stay Updated**: Machine learning is a rapidly evolving field. Keep learning about new techniques, algorithms, and best practices.\n",
      "\n",
      "9. **Specializations**: Depending on your interest, you might want to specialize in a particular area of machine learning such as natural language processing, computer vision, reinforcement learning, etc.\n",
      "\n",
      "Remember, mastering machine learning takes time and practice. Don't rush it and make sure to enjoy the learning process. Good luck!\n"
     ]
    }
   ],
   "source": [
    "print(bot.qa(\"Give me a roadmap to master machine learning.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, the ninth point suggests considering a specialization in a specific area of machine learning based on your interests. Here are some examples of specializations:\n",
      "\n",
      "- **Natural Language Processing (NLP)**: This area focuses on enabling computers to understand, interpret, and generate human language. NLP tasks include text classification, sentiment analysis, named entity recognition, part-of-speech tagging, machine translation, and question answering.\n",
      "\n",
      "- **Computer Vision**: This area deals with enabling machines to interpret and understand visual information from the world. Computer vision tasks include image classification, object detection, semantic segmentation, optical character recognition, and 3D vision.\n",
      "\n",
      "- **Reinforcement Learning**: This area involves training agents to take actions in an environment to maximize a reward signal. Reinforcement learning has applications in robotics, gaming, resource management, and navigation.\n",
      "\n",
      "- **Time Series Analysis**: This area deals with modeling and forecasting data that changes over time. Time series analysis has applications in finance, economics, signal processing, and weather forecasting.\n",
      "\n",
      "- **Bayesian Methods**: This area focuses on using Bayes' theorem to update probabilities given new evidence. Bayesian methods have applications in decision making, uncertainty estimation, and anomaly detection.\n",
      "\n",
      "By specializing in one or more of these areas, you can gain deeper expertise and become a subject matter expert in that domain. However, it's not necessary to specialize; many machine learning practitioners work on a wide range of problems and draw on various areas of machine learning.\n"
     ]
    }
   ],
   "source": [
    "print(bot.qa(\"Explain your nineth point.\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
