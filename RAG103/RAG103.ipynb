{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\">Pre-requisite: RAG101</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "We will address the context window issue we mentioned in RAG101. First, what is a context window? \n",
    "\n",
    "### Context WIndow\n",
    "\n",
    "According to [Lark](https://www.larksuite.com/en_us/topics/ai-glossary/context-window-llms):\n",
    "\n",
    "> A context window refers to a defined span of words within a text sequence that the Large Language Model (LLM) utilizes to extract contextual information.\n",
    "\n",
    "The larger the context window, the more words/tokens the LLM can utilize to extract contextual information. As a result, the model with a larger context window can have a longer conversation. The Mixtral-8x7B model has a theoretical attention span of 128k tokens, source: [Mixtral on ðŸ¤—](https://huggingface.co/docs/transformers/en/model_doc/mixtral#model-details). \n",
    "\n",
    "Think of a scenario where the context you passed into your prompt template is a whole textbook. If the tokens in the textbook are more than your model's context window, then your model can only consider the most recent tokens up to their maximum limit. \n",
    "\n",
    "What if there is a way to make your model consider only the relevant portion of your textbook? This is where embeddings come in.\n",
    "\n",
    "## Aim\n",
    "\n",
    "Our aim in RAG103 is to build a system that uses embeddings to extract the most relevant part of the provided context to answer a query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token, Tokenizer, Tokenization\n",
    "\n",
    "* **Token**: In Natural Language Processing (NLP), a token is a meaningful unit of text, such as a word, punctuation mark, or number, obtained by breaking down text through a process called tokenization.\n",
    "\n",
    "* **Tokenizer**: A tokenizer is an algorithm used in NLP to split text into tokens.\n",
    "\n",
    "* **Tokenization**: Tokenization is the process of dividing text into a sequence of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
