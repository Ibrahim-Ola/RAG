{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Load libraries:\n",
    "\n",
    "Note: See Appendix on how to get your Hugging Face ðŸ¤— API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import warnings\n",
    "import transformers\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate \n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "load_dotenv() # take environment variables from .env.\n",
    "api_key = os.getenv(\"Huggingface_API_key\")\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore') # ignore warnings\n",
    "\n",
    "# Set Transformer verbosity (only use this when you're sure your code is correct!)\n",
    "transformers.utils.logging.set_verbosity(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check available device\n",
    "\n",
    "Here, You are checking the device (`cuda`, `mps`, or `cpu`) available on your system. For Mac users, you will get either `cpu` or `mps`. For Windows or Linux users, you will get either `cpu` or `cuda`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Model\n",
    "\n",
    "Here, we would configure our and download the Mixtral-8x7B (M8x7B) model using Hugging Face's transformers. Setting `device_map=\"auto\"` first utilize the GPU(s) memory, then CPU memory if needed, and finally stores data on the disk when both memory types are full. Also, we are loading the 4-bits precision model to save memory.\n",
    "\n",
    "Link to M8x7B on ðŸ¤—: [Mixtral-8x7B](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85352dc3a5c240cc9926ab940fca2f7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\" # the model id on ðŸ¤—\n",
    "\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    token=api_key\n",
    ")\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    device_map='auto',\n",
    "    token=api_key,\n",
    "    load_in_4bit=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model set to evaluation mode.\n"
     ]
    }
   ],
   "source": [
    "# set model to evaluation mode\n",
    "\n",
    "model.eval()\n",
    "\n",
    "print(\"Model set to evaluation mode.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer\n",
    "\n",
    "We will instantiate a `tokenizer` designed to process natural language input by converting it into token lists compatible with the input layer of the M8x7B LLM. Note that we set `padding_side='left'` because we are working with a *decoder only* model. You can learn mode about decoder only models here on [Hugging Face](https://huggingface.co/learn/nlp-course/chapter1/6?fw=pt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    token=api_key,\n",
    "    padding_side='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction Format\n",
    "\n",
    "To get the most out of M8x7B, you must follow the instruction format as outlined by [MistralAi](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1#instruction-format). The instruction format for M8x7B is:\n",
    "\n",
    "```\n",
    "<s> [INST] Instruction [/INST] Model answer</s> [INST] Follow-up instruction [/INST]\n",
    "```\n",
    "\n",
    "Where:\n",
    "\n",
    "* `<s> `and `</s>` are special tokens used by MixtralAI to signify the beginning of string (BOS) and end of string (EOS).\n",
    "* Instruction is the user message.\n",
    "* Model answer is where the model response goes.\n",
    "* [/INST] and [INST] indicates the start and end of user messages.\n",
    "\n",
    "Note: for enforcing guardrails, prepend the instruction with your safety/syatem prompt. For this project, I will use the safety prompt used in the [Mistral 7B paper](https://arxiv.org/pdf/2310.06825.pdf).\n",
    "\n",
    "You can easily get the instruction form using ðŸ¤—'s `apply_chat_template()` method. Let's see an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Hello! [/INST]Hello. How can I help you today?</s>[INST] I'd like to show off how chat templating works! [/INST]\n"
     ]
    }
   ],
   "source": [
    "# ðŸ¤—'s Approach\n",
    "\n",
    "chat = [\n",
    "  {\"role\": \"user\", \"content\": \"Hello!\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"Hello. How can I help you today?\"},\n",
    "  {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n",
    "]\n",
    "\n",
    "print(tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's create a custom function that takes the user's prompt and dynamically converts it to M8x7B's format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_mixtral_template(instruction: str, safety_mode: bool = True) -> str:\n",
    "\n",
    "    if safety_mode:\n",
    "        safety_prompt = (\n",
    "            \"Always assist with care, respect, and truth. Respond with utmost utility yet securely. \"\n",
    "            \"Avoid harmful,unethical, prejudiced, or negative content. Ensure replies promote fairness and positivity.\"\n",
    "        )\n",
    "        \n",
    "        instruction = f\"{safety_prompt} {instruction}\"\n",
    "    \n",
    "    chat = [\n",
    "        {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n",
    "        {\"role\": \"user\", \"content\": instruction}\n",
    "    ]\n",
    "\n",
    "    hf_output = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=False)\n",
    "\n",
    "    return hf_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's define a function that formats our text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s>[INST] Always assist with care, respect, and truth. Respond with utmost utility yet securely. Avoid harmful,unethical, prejudiced, or negative content. Ensure replies promote fairness and positivity. I would like to book a flight to Paris. [/INST]\n"
     ]
    }
   ],
   "source": [
    "formatted_text=text_to_mixtral_template(\"I would like to book a flight to Paris.\", safety_mode=True)\n",
    "print(formatted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Text Generation Pipeline\n",
    "\n",
    "Let's create a text generation pipeline to plug into the LangChain API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_pipeline = transformers.pipeline(\n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,  \n",
    "    task='text-generation',\n",
    "    framework=\"pt\",\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=512,  \n",
    "    repetition_penalty=1.1, # \n",
    "    do_sample=True, \n",
    ")\n",
    "\n",
    "local_llm = HuggingFacePipeline(\n",
    "    pipeline=hf_pipeline,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup LangChain\n",
    "\n",
    "LangChain is an open source Python framework for building applications powered by Large Language Models (LLMs). Learn more about [LangChain](https://python.langchain.com/docs/get_started/introduction).\n",
    "\n",
    "We will be using the `PromptTemplate` and `LLMChain`.\n",
    "\n",
    "* [PromptTemplate](https://python.langchain.com/docs/modules/model_io/prompts/quick_start#prompttemplate): In LangChain, PromptTemplate is used to create a template for a string prompt. Note that you must use the correct model instruction format.\n",
    "* [LLMChain](https://api.python.langchain.com/en/stable/chains/langchain.chains.llm.LLMChain.html#langchain.chains.llm.LLMChain): In LangChain, a chain is used to link reusable components together. `LLMChain` is used to link prompt template, LLM, and other componenets togther."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " No, as of my knowledge up to this point, Twitter has not changed its name. It is still known as \"Twitter.\" If there have been any recent changes, I would recommend checking the latest news sources for the most accurate information. Is there anything else I can help you with?\n"
     ]
    }
   ],
   "source": [
    "input_text=\"{question}\"\n",
    "question = \"Does Twitter have a new name?\"\n",
    "template = text_to_mixtral_template(input_text, safety_mode=True)\n",
    "\n",
    "prompt_template = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt_template, llm=local_llm)\n",
    "result=llm_chain.invoke(question)\n",
    "print(result['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Create a Question and Answer Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_with_m8x7b(question: str, safety_mode: bool = True) -> str:\n",
    "    input_text=\"{question}\"\n",
    "    template = text_to_mixtral_template(input_text, safety_mode=safety_mode)\n",
    "\n",
    "    prompt_template = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "    llm_chain = LLMChain(prompt=prompt_template, llm=local_llm)\n",
    "    result=llm_chain.invoke(question)\n",
    "    return result['text'].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As of my last update, the President of Nigeria is Muhammadu Buhari. He has been in office since May 29, 2015. However, I recommend double-checking the most recent sources to confirm as this information could have changed.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_with_m8x7b(\"Who is the current president of Nigeria?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What did you notice?\n",
    "\n",
    "Our model's knowledge is out of date! We need to \"augment\" our model's knowledge with user-specific data. This is what RAG is all about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_with_context_to_mixtral_template(instruction: str, context: str, safety_mode: bool = True) -> str:\n",
    "\n",
    "    context_instruction = f\"Answer the following question using the provided context. \\n\\ncontext: {context}\"\n",
    "\n",
    "    if safety_mode:\n",
    "        safety_prompt = (\n",
    "            \"Always assist with care, respect, and truth. Respond with utmost utility yet securely. \"\n",
    "            \"Avoid harmful,unethical, prejudiced, or negative content. Ensure replies promote fairness and positivity.\"\n",
    "        )\n",
    "        \n",
    "        instruction = f\"{safety_prompt}\\n\\n{context_instruction} \\n\\nquestion: {instruction}\"\n",
    "\n",
    "    else:\n",
    "        instruction = f\"{context_instruction} \\n\\nquestion: {instruction}\"\n",
    "    \n",
    "    chat = [\n",
    "        {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n",
    "        {\"role\": \"user\", \"content\": instruction}\n",
    "    ]\n",
    "\n",
    "    return tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s>[INST] Always assist with care, respect, and truth. Respond with utmost utility yet securely. Avoid harmful,unethical, prejudiced, or negative content. Ensure replies promote fairness and positivity.\n",
      "\n",
      "Answer the following question using the provided context. \n",
      "\n",
      "context: It's sunny outside. \n",
      "\n",
      "question: What is the weather today? [/INST]\n"
     ]
    }
   ],
   "source": [
    "context=\"It's sunny outside.\"\n",
    "question = \"What is the weather today?\"\n",
    "formatted_text=text_with_context_to_mixtral_template(instruction=question, context=context, safety_mode=True)\n",
    "print(formatted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_with_context_m8x7b(question: str, context: str, safety_mode: bool = True) -> str:\n",
    "    input_text=\"{question}\"\n",
    "    input_context=\"{context}\"\n",
    "    template = text_with_context_to_mixtral_template(instruction=input_text, context=input_context, safety_mode=safety_mode)\n",
    "\n",
    "    prompt_template = PromptTemplate(template=template, input_variables=[\"question\", \"context\"])\n",
    "    llm_chain = LLMChain(prompt=prompt_template, llm=local_llm)\n",
    "    result=llm_chain.invoke({\"question\": question, \"context\": context})\n",
    "    return result['text'].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get Context from Wikipedia\n",
    "\n",
    "* Context about Nigeria's president: [click here](https://en.wikipedia.org/wiki/President_of_Nigeria#Fourth_Republic_(1999%E2%80%93present))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nigeria_context=\"\"\"\n",
    "On 29 May 1999, General Abdulsalami Abubakar stepped down, and handed over power to a former military head of state, Olusegun Obasanjo, after being elected some months prior. Obasanjo served two terms in office.\n",
    "On 29 May 2007, Umaru Musa Yar'Adua was sworn in as president of the Federal Republic of Nigeria and the 13th head of state completing the first successful transition of power, \n",
    "from one democratically elected president to another in Nigeria. Yar'Adua died on 5 May 2010 at the presidential villa, in Abuja, Nigeria, becoming the second head of state to die there after General Sani Abacha.\n",
    "On 6 May 2010, Vice President Goodluck Jonathan was sworn in as president of the Federal Republic of Nigeria and the 14th head of state.\n",
    "On 29 May 2015, Muhammadu Buhari, a former military head of state was sworn in as president of the Federal Republic of Nigeria and the 15th head of state after winning the general election. He also served two terms in office.\n",
    "On 29 May 2023, Bola Tinubu was sworn in as president of the Federal Republic of Nigeria and the 16th head of state after winning the 2023 Nigerian general election.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current president of Nigeria is Bola Tinubu. He was sworn in as the 16th head of state on May 29, 2023, after winning the 2023 Nigerian general election. Prior to his presidency, he served as the Governor of Lagos State from 1999 to 2007. He is a member of the All Progressives Congress (APC) party.\n",
      "\n",
      "It is important to note that this information is based on the context provided and may vary if new events occur or new information becomes available.\n"
     ]
    }
   ],
   "source": [
    "print(qa_with_context_m8x7b(question=\"Who is the current president of Nigeria?\", context=nigeria_context))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Context about Twitter's name: [click here](https://en.wikipedia.org/wiki/Twitter#:~:text=Although%20the%20service%20is%20now,name%20redirecting%20to%20that%20address.&text=The%20service%20is%20owned%20by,the%20successor%20of%20Twitter%2C%20Inc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_context=\"\"\"\n",
    "X, formerly (and still colloquially) known as Twitter, is a social media website based in the United States. \n",
    "With over 500 million users, it is one of the world's largest social networks. \n",
    "Users can share and post text messages, images, and videos known historically as \"tweets\". \n",
    "X also includes direct messaging, video and audio calling, bookmarks, lists and communities, and Spaces, a social audio feature. \n",
    "Users can vote on context added by approved users using the Community Notes feature. \n",
    "Although the service is now called X, the primary URL remains twitter.com as of January 2024, with the x.com domain name redirecting to that address.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, Twitter has changed its name to X. However, the primary URL for the service remains twitter.com as of January 2024. The x.com domain name redirects to that address.\n"
     ]
    }
   ],
   "source": [
    "print(qa_with_context_m8x7b(question=\"Does Twitter have a new name?\", context=twitter_context))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it Together\n",
    "\n",
    "Now that we have seen the componenets, let put our codes together in a script and call it like a package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spring rain falls gently,\n",
      "Buds bloom, waking from winter's sleep,\n",
      "Nature's rebirth sings.\n"
     ]
    }
   ],
   "source": [
    "from utils.rag101 import NaiveRAG\n",
    "\n",
    "rag = NaiveRAG(model=None, tokenizer=None) ## initialize RAG\n",
    "\n",
    "## question without context\n",
    "\n",
    "question = \"Write me a haiku about the weather.\"\n",
    "\n",
    "response=rag.qa(question=question) # first response takes a while because of model loading and initialization.\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X was formally known as Twitter. Even though its official name has been changed, the primary URL for the site is still twitter.com as of January 2024. The domain name x.com redirects to the twitter.com address.\n"
     ]
    }
   ],
   "source": [
    "question=\"What was X formally known as?\"\n",
    "response=rag.qa_with_context(question=question, context=twitter_context)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Thoughts\n",
    "\n",
    "We have a chatbot that works completely offline/on our PC. But there are a few problem.\n",
    "\n",
    "1. Our chat does not have memory.\n",
    "2. Copy and pasting contents is not an efficient way to handle additional context. What if the content is longer than the contenxt window?\n",
    "\n",
    "We will address Memory issue in RAG102 and context window issue in RAG103."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "### Getting Hugging Face API key\n",
    "\n",
    "1. **Create an Account**: Sign up for an account on [the Hugging Face website](https://huggingface.co).\n",
    "2. **Generate an API Key**: Once logged in, go to your account settings, click on Access Tokens, and generate a new API key."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoskln",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
