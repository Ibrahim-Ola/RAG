{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\">Pre-requisite: RAG101</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "We will address the context window issue we mentioned in RAG101. First, what is a context window? \n",
    "\n",
    "### Context WIndow\n",
    "\n",
    "According to [Lark](https://www.larksuite.com/en_us/topics/ai-glossary/context-window-llms):\n",
    "\n",
    "> A context window refers to a defined span of words within a text sequence that the Large Language Model (LLM) utilizes to extract contextual information.\n",
    "\n",
    "The larger the context window, the more words/tokens the LLM can utilize to extract contextual information. As a result, the model with a larger context window can have a longer conversation. The Mixtral-8x7B model has a theoretical attention span of 128k tokens, source: [Mixtral on ü§ó](https://huggingface.co/docs/transformers/en/model_doc/mixtral#model-details). \n",
    "\n",
    "Think of a scenario where the context you passed into your prompt template is a whole textbook. If the tokens in the textbook are more than your model's context window, then your model can only consider the most recent tokens up to their maximum limit. \n",
    "\n",
    "What if there is a way to make your model consider only the relevant portion of your textbook? This is where embeddings come in.\n",
    "\n",
    "## Aim\n",
    "\n",
    "Our aim in RAG103 is to build a system that uses embeddings to extract the most relevant part of the provided context to answer a query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token, Tokenizer, Tokenization\n",
    "\n",
    "* **Token**: In Natural Language Processing (NLP), a token is a meaningful unit of text, such as a word, punctuation mark, or number, obtained by breaking down text through a process called tokenization.\n",
    "\n",
    "* **Tokenizer**: A tokenizer is an algorithm used in NLP to split text into tokens.\n",
    "\n",
    "* **Tokenization**: Tokenization is the process of dividing text into a sequence of tokens.\n",
    "\n",
    "Let's see a sample tokenization using the Mixtral-8x7B tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "from dotenv import load_dotenv\n",
    "from utils.rag101 import NaiveRAG\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "load_dotenv() # take environment variables from .env.\n",
    "api_key = os.getenv(\"Huggingface_API_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is how the sequence 'Using a Transformer network is simple.' is tokenized using the Mixtral-8x7B tokenizer:\n",
      "\n",
      "Token 0: ‚ñÅUsing\n",
      "Token 1: ‚ñÅa\n",
      "Token 2: ‚ñÅTrans\n",
      "Token 3: former\n",
      "Token 4: ‚ñÅnetwork\n",
      "Token 5: ‚ñÅis\n",
      "Token 6: ‚ñÅsimple\n",
      "Token 7: .\n",
      "\n",
      "Tokenization complete.\n"
     ]
    }
   ],
   "source": [
    "rag = NaiveRAG(model=None, tokenizer=None) ## initialize RAG\n",
    "\n",
    "sequence = \"Using a Transformer network is simple.\"\n",
    "tokens = rag.tokenizer.tokenize(sequence)\n",
    "\n",
    "print(f\"This is how the sequence '{sequence}' is tokenized using the Mixtral-8x7B tokenizer:\\n\")\n",
    "\n",
    "for id, token in enumerate(tokens):\n",
    "    print(f\"Token {id}: {token}\")\n",
    "\n",
    "print(\"\\nTokenization complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can learn more about tokenizers [here](https://huggingface.co/learn/nlp-course/en/chapter2/4). Now that we are familiar with tokenizers and context windows, let's discuss embeddings.\n",
    "\n",
    "## What is an embedding?\n",
    "\n",
    "In NLP, an embedding is a representation of text where words, phrases, or even entire documents are mapped to vectors of real numbers. In our implementation, we will use the [BGE](https://huggingface.co/BAAI) open-source embedding model from [Hugging Face Hub](https://huggingface.co/docs/hub/index). The Hugging Face Hub is home to more than 350k open-source models. I have chosen the [BGE](https://huggingface.co/BAAI) embedding model because it is one of the [best open-source embedding models](https://huggingface.co/spaces/mteb/leaderboard). \n",
    "\n",
    "\n",
    "Let's explore how embeddings work!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_id = \"BAAI/bge-small-en-v1.5\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "\n",
    "hf = HuggingFaceBgeEmbeddings(\n",
    "    model_name=embedding_model_id, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = hf.embed_query(\"This is an introduction to Embeddings.\")\n",
    "len(embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
