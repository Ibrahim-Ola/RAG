{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Load libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from utils.rag102 import ChatBot\n",
    "from utils.rag101 import NaiveRAG\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "We have seen a naive implementation of RAG in RAG101. However, one of the problems we metioned was about our chatbot not having memory. Consider the conversations below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spring rain falls gently,\n",
      "Buds bloom, waking from winter's sleep,\n",
      "Nature reborn, anew.\n"
     ]
    }
   ],
   "source": [
    "rag = NaiveRAG(model=None, tokenizer=None) ## initialize RAG\n",
    "\n",
    "## question without context\n",
    "question = \"Write me a haiku about the weather.\"\n",
    "\n",
    "response=rag.qa(question=question) # first response takes a while because of model loading and initialization.\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, the text you provided is a haiku. A haiku is a form of traditional Japanese poetry that consists of three lines with a 5-7-5 syllable pattern. The first line has five syllables, the second line has seven, and the third line has five again. The text you gave \"Always assist... positive.\" follows this syllable pattern: 5-7-5. It also conveys a message about providing helpful and positive responses.\n"
     ]
    }
   ],
   "source": [
    "follow_up_question = \"Are you sure this is a haiku? Analyze it for me.\"\n",
    "\n",
    "response=rag.qa(question=follow_up_question) \n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I apologize if my previous response did not meet your expectations. I will make sure to always assist you with care, respect, and honesty. I will avoid any harmful, unethical, prejudiced, or negative content, and ensure that my responses promote fairness and positivity. I am here to provide the most accurate and helpful information possible. If you have a specific question or problem, please let me know and I will do my best to assist you.\n",
      "\n",
      "Regarding your statement \"your analysis is wrong\", I would need more context in order to understand what you are referring to and address any potential misunderstanding. My goal is to provide correct and relevant information, so please let me know if there is anything specific that you would like me to clarify or correct.\n"
     ]
    }
   ],
   "source": [
    "follow_up_question = \"No, your analysis is wrong.\"\n",
    "\n",
    "response=rag.qa(question=follow_up_question) \n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What did you Notice?\n",
    "\n",
    "Our model doesn't remember our previous conversations. By design, LLMs doesn't have memory. We must build memory into our system to enable a conversation. We can achieve this if our LLM has access to previous (or at least a window of previous) conversations. LangChain supports a number of memory algorithms. For RAG102, we will concentrate on `ConversationBufferMemory`.\n",
    "\n",
    "* [ConversationBufferMemory](https://python.langchain.com/docs/modules/memory/types/buffer): This is the simplest form of memory. It stores all conversations between the AI and the USER and passes them into the prompt template when the USER asks a new question. You can think of this history as `context` in our previous implementation (RAG101), but here, the context is the chat history. Now let's implement the `ConversationBufferMemory` in LangChain. We will start by modifying the mixtral instruction format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_with_context_to_mixtral_template(instruction: str, history: str, safety_mode: bool = True) -> str:\n",
    "\n",
    "    history_instruction = f\"The following is a conversation between a human and an AI assistant.\"\n",
    "\n",
    "    if safety_mode:\n",
    "        safety_prompt = (\n",
    "            \"The AI assistant always assist with care, respect, and truth. The AI assistant respond with utmost utility yet securely. \"\n",
    "            \"The AI assistant avoids harmful, unethical, prejudiced, or negative content and ensures replies promote fairness and positivity.\"\n",
    "        )\n",
    "\n",
    "        instruction = f\"{history_instruction} {safety_prompt}\\n\\nCurrent conversation: {history}\\n\\nHuman: {instruction}\"\n",
    "\n",
    "    else:\n",
    "        instruction = f\"{history_instruction}\\n\\nCurrent conversation: {history} \\n\\nHuman: {instruction}\"\n",
    "    \n",
    "    chat = [\n",
    "        {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n",
    "        {\"role\": \"user\", \"content\": instruction}\n",
    "    ]\n",
    "\n",
    "    return rag.tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s>[INST] The following is a conversation between a human and an AI assistant.\n",
      "\n",
      "Current conversation: {history} \n",
      "\n",
      "Human: {input} [/INST]\n"
     ]
    }
   ],
   "source": [
    "history=\"{history}\"\n",
    "question = \"{input}\"\n",
    "formatted_text=text_with_context_to_mixtral_template(instruction=question, history=history, safety_mode=False)\n",
    "print(formatted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat with Mixtral8x7B\n",
    "\n",
    "* Now that the instruction format is set up let's have some conversations. I'll turn on verbose so that you can see what's going on under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_prefix(text, prefix=\"AI Assistant:\"):\n",
    "    pattern = r'^' + re.escape(prefix) + r'\\s*'\n",
    "    return re.sub(pattern, '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s>[INST] The following is a conversation between a human and an AI assistant. The AI assistant always assist with care, respect, and truth. The AI assistant respond with utmost utility yet securely. The AI assistant avoids harmful, unethical, prejudiced, or negative content and ensures replies promote fairness and positivity.\n",
      "\n",
      "Current conversation: \n",
      "\n",
      "Human: hi [/INST]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Hello! How can I assist you today? Is there something specific you would like to know or discuss? I'm here to help and provide accurate information in a friendly and respectful manner.\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferMemory(memory_key=\"history\") # let's define the memory\n",
    "\n",
    "input_text=\"{input}\"\n",
    "chat_history=\"{history}\"\n",
    "template = text_with_context_to_mixtral_template(instruction=input_text, history=history) # mixtral's instruction format\n",
    "\n",
    "# let's define the prompt using LangChain's PromptTemplate\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# let's link componenet with LLMChain\n",
    "conversation = LLMChain(\n",
    "    llm=rag.model,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "def remove_prefix(text, prefix=\"AI Assistant:\"):\n",
    "\n",
    "    \"\"\"\n",
    "    Removes the prefix from the text.\n",
    "    \"\"\"\n",
    "\n",
    "    pattern = r'^' + re.escape(prefix) + r'\\s*'\n",
    "    return re.sub(pattern, '', text)\n",
    "\n",
    "## let's chat\n",
    "response=conversation({\"input\": \"hi\"})\n",
    "print(remove_prefix(response['text'].strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s>[INST] The following is a conversation between a human and an AI assistant. The AI assistant always assist with care, respect, and truth. The AI assistant respond with utmost utility yet securely. The AI assistant avoids harmful, unethical, prejudiced, or negative content and ensures replies promote fairness and positivity.\n",
      "\n",
      "Current conversation: Human: hi\n",
      "AI:  AI Assistant: Hello! How can I assist you today? Is there something specific you would like to know or discuss? I'm here to help and provide accurate information in a friendly and respectful manner.\n",
      "\n",
      "Human: Write me a haiku about USA. [/INST]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Of course! Here's a haiku for you:\n",
      "\n",
      "Endless prairies meet,\n",
      "Mountains touch the sky so blue,\n",
      "USA, land of dreams.\n",
      "\n",
      "Would you like to learn more about the United States or any other topic? I'm here to help.\n"
     ]
    }
   ],
   "source": [
    "response=conversation({\"input\": \"Write me a haiku about USA.\"})\n",
    "print(remove_prefix(response['text'].strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s>[INST] The following is a conversation between a human and an AI assistant. The AI assistant always assist with care, respect, and truth. The AI assistant respond with utmost utility yet securely. The AI assistant avoids harmful, unethical, prejudiced, or negative content and ensures replies promote fairness and positivity.\n",
      "\n",
      "Current conversation: Human: hi\n",
      "AI:  AI Assistant: Hello! How can I assist you today? Is there something specific you would like to know or discuss? I'm here to help and provide accurate information in a friendly and respectful manner.\n",
      "Human: Write me a haiku about USA.\n",
      "AI:  Of course! Here's a haiku for you:\n",
      "\n",
      "Endless prairies meet,\n",
      "Mountains touch the sky so blue,\n",
      "USA, land of dreams.\n",
      "\n",
      "Would you like to learn more about the United States or any other topic? I'm here to help.\n",
      "\n",
      "Human: Thank you! Analyze the haiku for me. [/INST]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Sure! A haiku is a form of traditional Japanese poetry that consists of three lines with a 5-7-5 syllable pattern. It often captures a moment, feeling, or scene in nature.\n",
      "\n",
      "In the haiku I wrote about the USA, \"Endless prairies meet\" describes the vast open spaces and agricultural regions of the country. The second line, \"Mountains touch the sky so blue,\" highlights the majestic mountain ranges and clear, bright skies found across the USA. And the third line, \"USA, land of dreams,\" reflects the country's history as a place where people from all over the world have come in search of opportunity and a better life.\n",
      "\n",
      "Overall, the haiku aims to capture the beauty, diversity, and spirit of the United States in just a few words. Let me know if you have any questions about the haiku or if there's anything else you'd like to talk about. I'm here to help.\n"
     ]
    }
   ],
   "source": [
    "response=conversation({\"input\": \"Thank you! Analyze the haiku for me.\"})\n",
    "print(remove_prefix(response['text'].strip()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can see how this conversation unfolds. For a chatbot application, we would set `verbose=False`. Other memory algorithm supported by LangChain include:\n",
    "\n",
    "1. [ConversationBufferWindowMemory](https://python.langchain.com/docs/modules/memory/types/buffer_window).\n",
    "2. [ConversationEntityMemory](https://python.langchain.com/docs/modules/memory/types/entity_summary_memory).\n",
    "3. [ConversationKGMemory](https://python.langchain.com/docs/modules/memory/types/kg).\n",
    "4. [ConversationSummaryMemory](https://python.langchain.com/docs/modules/memory/types/summary).\n",
    "5. [ConversationSummaryBufferMemory](https://python.langchain.com/docs/modules/memory/types/summary_buffer).\n",
    "6. [ConversationTokenBufferMemory](https://python.langchain.com/docs/modules/memory/types/token_buffer).\n",
    "7. [VectorStoreRetrieverMemory](https://python.langchain.com/docs/modules/memory/types/vectorstore_retriever_memory)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it Together\n",
    "\n",
    "We will create a new script called [rag102](https://github.com/Ibrahim-Ola/RAG/blob/main/utils/rag102.py) that puts all the code together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot=ChatBot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
