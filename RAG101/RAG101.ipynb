{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Load libraries:\n",
    "\n",
    "Note: See Appendix on how to get your Hugging Face ðŸ¤— API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "from dotenv import load_dotenv\n",
    "from langchain import HuggingFacePipeline, PromptTemplate, LLMChain\n",
    "\n",
    "load_dotenv() # take environment variables from .env.\n",
    "api_key = os.getenv(\"Huggingface_API_key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check available device\n",
    "\n",
    "Here, You are checking the device (`cuda`, `mps`, or `cpu`) available on your system. For Mac users, you will get either `cpu` or `mps`. For Windows or Linux users, you will get either `cpu` or `cuda`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Model\n",
    "\n",
    "Here, we would configure our and download the Mixtral-8x7B (M7B) model using Hugging Face's transformers. Setting `device_map=\"auto\"` first utilize the GPU(s) memory, then CPU memory if needed, and finally stores data on the disk when both memory types are full. Also, we are loading the 4-bits precision model to save memory.\n",
    "\n",
    "Link to M7B on ðŸ¤—: [Mixtral-8x7B](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "831d546166714334af8cde6e99a10abb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\" # the model id on ðŸ¤—\n",
    "\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    token=api_key\n",
    ")\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    device_map='auto',\n",
    "    token=api_key,\n",
    "    load_in_4bit=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model set to evaluation mode.\n"
     ]
    }
   ],
   "source": [
    "# set model to evaluation mode\n",
    "\n",
    "model.eval()\n",
    "\n",
    "print(\"Model set to evaluation mode.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer\n",
    "\n",
    "We will instantiate a `tokenizer` designed to process natural language input by converting it into token lists compatible with the input layer of the MS7 LLM. Note that we set `padding_side='left'` because we are working with a *decoder only* model. You can learn mode about decoder only models here on [Hugging Face](https://huggingface.co/learn/nlp-course/chapter1/6?fw=pt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    token=api_key,\n",
    "    padding_side='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. \n",
      "If you don't know the answer to a question, please don't share false information. [/INST] What is the name of the current president of Nigeria? </s>\n"
     ]
    }
   ],
   "source": [
    "def text_to_mixtral_template(instruction: str, system_prompt: str = None) -> str:\n",
    "\n",
    "    begin_of_string=\"<s> [INST]\"\n",
    "    end_of_string=\"</s>\"\n",
    "\n",
    "    if not system_prompt:\n",
    "        system_prompt = (\n",
    "            \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. \"\n",
    "            \"\\nIf you don't know the answer to a question, \"\n",
    "            \"please don't share false information.\"\n",
    "        )\n",
    "\n",
    "        mixtral_prompt_template = f\"{begin_of_string} {system_prompt} [/INST] {instruction} {end_of_string}\"\n",
    "\n",
    "    else:\n",
    "        mixtral_prompt_template = f\"{begin_of_string} {system_prompt} [/INST] {instruction} {end_of_string}\"\n",
    "\n",
    "    return mixtral_prompt_template\n",
    "\n",
    "print(text_to_mixtral_template(\"What is the name of the current president of Nigeria?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_pipeline = transformers.pipeline(\n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,  \n",
    "    task='text-generation',\n",
    "    framework=\"pt\",\n",
    "    temperature=0.3,\n",
    "    max_new_tokens=512,  \n",
    "    repetition_penalty=1.1,\n",
    "    do_sample=True, \n",
    ")\n",
    "\n",
    "local_llm =HuggingFacePipeline(\n",
    "    pipeline=hf_pipeline,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    }
   ],
   "source": [
    "input_text=\"{question}\"\n",
    "question: str = \"What is the name of the current president of Nigeria?\"\n",
    "template = text_to_mixtral_template(input_text)\n",
    "\n",
    "prompt_template = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt_template, llm=local_llm)\n",
    "result=llm_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    }
   ],
   "source": [
    "input_text=\"{question}\"\n",
    "question: str = \"What is Twitter's new name?\"\n",
    "template = text_to_mixtral_template(input_text)\n",
    "\n",
    "prompt_template = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt_template, llm=local_llm)\n",
    "result=llm_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of now, Twitter has not announced any changes to their name. They are still known as Twitter, Inc. If there are any updates in the future, I will make sure to provide you with accurate and reliable information. Is there anything else I can assist you with?\n"
     ]
    }
   ],
   "source": [
    "print_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_result(result):\n",
    "    formatted_result = result['text'][result['text'].find(\"\\n\"):].strip()\n",
    "    print(formatted_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of my last update, the current president of Nigeria is Muhammadu Buhari. He has been in office since May 29, 2015. However, I recommend checking the most recent sources to confirm as this information might have changed.\n"
     ]
    }
   ],
   "source": [
    "print(result['text'][result['text'].find(\"\\n\"):].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template=PromptTemplate(\n",
    "    input_variables=[\"question\", \"context\"],\n",
    "    template=\"\"\"<s> [INST] You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. \n",
    "    If you don't know the answer to a question, please don't share false information. [/INST] Answer the following question using the provided context. \n",
    "    \\n context: {context} \\n\\n question: {question} </s>\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "context=\"\"\"\n",
    "On 29 May 1999, General Abdulsalami Abubakar stepped down, and handed over power to a former military head of state, Olusegun Obasanjo, after being elected some months prior. Obasanjo served two terms in office.\n",
    "\n",
    "On 29 May 2007, Umaru Musa Yar'Adua was sworn in as president of the Federal Republic of Nigeria and the 13th head of state completing the first successful transition of power, from one democratically elected president to another in Nigeria. Yar'Adua died on 5 May 2010 at the presidential villa, in Abuja, Nigeria, becoming the second head of state to die there after General Sani Abacha.\n",
    "\n",
    "On 6 May 2010, Vice President Goodluck Jonathan was sworn in as president of the Federal Republic of Nigeria and the 14th head of state.\n",
    "\n",
    "On 29 May 2015, Muhammadu Buhari, a former military head of state was sworn in as president of the Federal Republic of Nigeria and the 15th head of state after winning the general election. He also served two terms in office.\n",
    "\n",
    "On 29 May 2023, Bola Tinubu was sworn in as president of the Federal Republic of Nigeria and the 16th head of state after winning the 2023 Nigerian general election.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    }
   ],
   "source": [
    "llm_chain = LLMChain(prompt=prompt_template, llm=local_llm)\n",
    "result=llm_chain.invoke({\"question\": question, \"context\": context})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current president of Nigeria is Bola Tinubu, who was sworn in as president on 29 May 2023.\n"
     ]
    }
   ],
   "source": [
    "print_result(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoskln",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
