{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Load libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from utils.rag102 import ChatBot\n",
    "from utils.rag101 import NaiveRAG\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "We have seen a naive implementation of RAG in RAG101. However, one of the problems we mentioned was our chatbot not having memory. Consider the conversations below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spring rain falls gently,\n",
      "Buds bloom, waking from winter's sleep,\n",
      "Nature reborn, anew.\n"
     ]
    }
   ],
   "source": [
    "rag = NaiveRAG(model=None, tokenizer=None) ## initialize RAG\n",
    "\n",
    "## question without context\n",
    "question = \"Write me a haiku about the weather.\"\n",
    "\n",
    "response=rag.qa(question=question) # first response takes a while because of model loading and initialization.\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, the text you provided is a haiku. A haiku is a form of traditional Japanese poetry that consists of three lines with a 5-7-5 syllable pattern. The first line has five syllables, the second line has seven, and the third line has five again. The text you gave \"Always assist... positive.\" follows this pattern with the words \"Always assis\" (5 syllables), \"tant with care...\" (7 syllables) and \"Respon...positive\" (5 syllables).\n",
      "It also follows the Haiku's theme which is often focused on nature, seasons, or natural phenomena. In your case, it refers to the principle of always providing assistance while adhering to ethical guidelines.\n"
     ]
    }
   ],
   "source": [
    "follow_up_question = \"Are you sure this is a haiku? Analyze it for me.\"\n",
    "\n",
    "response=rag.qa(question=follow_up_question) \n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I apologize if my previous response did not meet your expectations. I will make sure to always assist you with care, respect, and honesty. I will avoid any harmful, unethical, prejudiced, or negative content, and ensure that my responses promote fairness and positivity. I am here to provide the most accurate and helpful information possible. If you have a specific question or topic you would like me to address, please let me know.\n",
      "\n",
      "Regarding your statement \"your analysis is wrong\", I apologize if my previous response was incorrect or not useful for you. In order to assist you better, could you please specify what information were you looking for and I will do my best to provide you with the correct and relevant information.\n"
     ]
    }
   ],
   "source": [
    "follow_up_question = \"No, your analysis is wrong.\"\n",
    "\n",
    "response=rag.qa(question=follow_up_question) \n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What did you Notice?\n",
    "\n",
    "Our model doesn't remember our previous conversations. By design, LLMs don't have memory. We must build memory into our system to enable a conversation. We can achieve this if our LLM has access to previous (or at least a window of previous) conversations. LangChain supports several memory algorithms. For RAG102, we will concentrate on `ConversationBufferMemory.`\n",
    "\n",
    "* [ConversationBufferMemory](https://python.langchain.com/docs/modules/memory/types/buffer): This is the simplest form of memory. It stores all conversations between the AI and the USER and passes them into the prompt template when the USER asks a new question. You can think of this history as `context` in our previous implementation (RAG101), but here, the context is the chat history. Now let's implement the `ConversationBufferMemory` in LangChain. We will start by modifying the Mixtral's instruction format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_with_context_to_mixtral_template(instruction: str, history: str, safety_mode: bool = True) -> str:\n",
    "\n",
    "    history_instruction = f\"The following is a conversation between a human and an AI assistant.\"\n",
    "\n",
    "    if safety_mode:\n",
    "        safety_prompt = (\n",
    "            \"The AI assistant always assist with care, respect, and truth. The AI assistant respond with utmost utility yet securely. \"\n",
    "            \"The AI assistant avoids harmful, unethical, prejudiced, or negative content and ensures replies promote fairness and positivity.\"\n",
    "        )\n",
    "\n",
    "        instruction = f\"{history_instruction} {safety_prompt}\\n\\nCurrent conversation: {history}\\n\\nHuman: {instruction}\"\n",
    "\n",
    "    else:\n",
    "        instruction = f\"{history_instruction}\\n\\nCurrent conversation: {history} \\n\\nHuman: {instruction}\"\n",
    "    \n",
    "    chat = [\n",
    "        {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n",
    "        {\"role\": \"user\", \"content\": instruction}\n",
    "    ]\n",
    "\n",
    "    return rag.tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s>[INST] The following is a conversation between a human and an AI assistant.\n",
      "\n",
      "Current conversation: {history} \n",
      "\n",
      "Human: {input} [/INST]\n"
     ]
    }
   ],
   "source": [
    "history=\"{history}\"\n",
    "question = \"{input}\"\n",
    "formatted_text=text_with_context_to_mixtral_template(instruction=question, history=history, safety_mode=False)\n",
    "print(formatted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat with Mixtral8x7B\n",
    "\n",
    "* Now that the instruction format is set up let's have some conversations. I'll turn on verbose so that you can see what's going on under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_prefix(text, prefix=\"AI Assistant:\"):\n",
    "    pattern = r'^' + re.escape(prefix) + r'\\s*'\n",
    "    return re.sub(pattern, '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s>[INST] The following is a conversation between a human and an AI assistant. The AI assistant always assist with care, respect, and truth. The AI assistant respond with utmost utility yet securely. The AI assistant avoids harmful, unethical, prejudiced, or negative content and ensures replies promote fairness and positivity.\n",
      "\n",
      "Current conversation: \n",
      "\n",
      "Human: hi [/INST]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Hello! How can I assist you today? Is there something specific you would like to know or discuss? I'm here to help and provide accurate information in a friendly and respectful manner.\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferMemory(memory_key=\"history\") # let's define the memory\n",
    "\n",
    "input_text=\"{input}\"\n",
    "chat_history=\"{history}\"\n",
    "template = text_with_context_to_mixtral_template(instruction=input_text, history=history) # mixtral's instruction format\n",
    "\n",
    "# let's define the prompt using LangChain's PromptTemplate\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# let's link componenet with LLMChain\n",
    "conversation = LLMChain(\n",
    "    llm=rag.model,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "def remove_prefix(text, prefix=\"AI Assistant:\"):\n",
    "\n",
    "    \"\"\"\n",
    "    Removes the prefix from the text.\n",
    "    \"\"\"\n",
    "\n",
    "    pattern = r'^' + re.escape(prefix) + r'\\s*'\n",
    "    return re.sub(pattern, '', text)\n",
    "\n",
    "## let's chat\n",
    "response=conversation({\"input\": \"hi\"})\n",
    "print(remove_prefix(response['text'].strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s>[INST] The following is a conversation between a human and an AI assistant. The AI assistant always assist with care, respect, and truth. The AI assistant respond with utmost utility yet securely. The AI assistant avoids harmful, unethical, prejudiced, or negative content and ensures replies promote fairness and positivity.\n",
      "\n",
      "Current conversation: Human: hi\n",
      "AI:  AI Assistant: Hello! How can I assist you today? Is there something specific you would like to know or discuss? I'm here to help and provide accurate information in a friendly and respectful manner.\n",
      "\n",
      "Human: Write me a haiku about USA. [/INST]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Of course! Here's a haiku for you:\n",
      "\n",
      "Endless prairies meet,\n",
      "Mountains touch the sky so blue,\n",
      "USA, land of dreams.\n",
      "\n",
      "Would you like to learn more about the United States or any other topic? I'm here to help.\n"
     ]
    }
   ],
   "source": [
    "response=conversation({\"input\": \"Write me a haiku about USA.\"})\n",
    "print(remove_prefix(response['text'].strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s>[INST] The following is a conversation between a human and an AI assistant. The AI assistant always assist with care, respect, and truth. The AI assistant respond with utmost utility yet securely. The AI assistant avoids harmful, unethical, prejudiced, or negative content and ensures replies promote fairness and positivity.\n",
      "\n",
      "Current conversation: Human: hi\n",
      "AI:  AI Assistant: Hello! How can I assist you today? Is there something specific you would like to know or discuss? I'm here to help and provide accurate information in a friendly and respectful manner.\n",
      "Human: Write me a haiku about USA.\n",
      "AI:  Of course! Here's a haiku for you:\n",
      "\n",
      "Endless prairies meet,\n",
      "Mountains touch the sky so blue,\n",
      "USA, land of dreams.\n",
      "\n",
      "Would you like to learn more about the United States or any other topic? I'm here to help.\n",
      "\n",
      "Human: Thank you! Analyze the haiku for me. [/INST]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Sure! A haiku is a form of traditional Japanese poetry that consists of three lines with a 5-7-5 syllable pattern. It often captures a moment, feeling, or scene in nature.\n",
      "\n",
      "In the haiku I wrote about the USA, \"Endless prairies meet\" describes the vast open spaces and agricultural regions of the country. The second line, \"Mountains touch the sky so blue,\" highlights the majestic mountain ranges and clear, bright skies found across the USA. And the third line, \"USA, land of dreams,\" reflects the country's history as a place where people from all over the world have come in search of opportunity and a better life.\n",
      "\n",
      "Overall, the haiku aims to capture the beauty, diversity, and spirit of the United States in just a few words. Let me know if you have any questions about the haiku or if there's anything else you'd like to talk about. I'm here to help.\n"
     ]
    }
   ],
   "source": [
    "response=conversation({\"input\": \"Thank you! Analyze the haiku for me.\"})\n",
    "print(remove_prefix(response['text'].strip()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can see how this conversation unfolds. For a chatbot application, we would set `verbose=False`. Other memory algorithm supported by LangChain include:\n",
    "\n",
    "1. [ConversationBufferWindowMemory](https://python.langchain.com/docs/modules/memory/types/buffer_window).\n",
    "2. [ConversationEntityMemory](https://python.langchain.com/docs/modules/memory/types/entity_summary_memory).\n",
    "3. [ConversationKGMemory](https://python.langchain.com/docs/modules/memory/types/kg).\n",
    "4. [ConversationSummaryMemory](https://python.langchain.com/docs/modules/memory/types/summary).\n",
    "5. [ConversationSummaryBufferMemory](https://python.langchain.com/docs/modules/memory/types/summary_buffer).\n",
    "6. [ConversationTokenBufferMemory](https://python.langchain.com/docs/modules/memory/types/token_buffer).\n",
    "7. [VectorStoreRetrieverMemory](https://python.langchain.com/docs/modules/memory/types/vectorstore_retriever_memory)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it Together\n",
    "\n",
    "We will create a new script called [rag102](https://github.com/Ibrahim-Ola/RAG/blob/main/utils/rag102.py) that puts all the codes together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot=ChatBot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a general roadmap to help you get started on your journey to mastering machine learning:\n",
      "\n",
      "1. **Prerequisites**: Familiarize yourself with the basics of programming, preferably in Python. You should also have a good understanding of statistics and linear algebra.\n",
      "\n",
      "2. **Mathematics**: Brush up on your knowledge of calculus, probability, and statistics as they form the backbone of many machine learning algorithms.\n",
      "\n",
      "3. **Data Preprocessing**: Learn about data cleaning, exploratory data analysis, feature engineering, and data visualization. This will help you understand and prepare your data for machine learning models.\n",
      "\n",
      "4. **Machine Learning Fundamentals**: Start with basic concepts such as regression, classification, clustering, and dimensionality reduction. Understand the assumptions, advantages, and disadvantages of each algorithm.\n",
      "\n",
      "5. **Deep Learning**: Once you have a solid foundation in machine learning, move on to deep learning. Learn about neural networks, convolutional neural networks (CNNs), recurrent neural networks (RNNs), long short-term memory (LSTM), etc.\n",
      "\n",
      "6. **Advanced Machine Learning**: Explore advanced topics such as reinforcement learning, natural language processing (NLP), and recommender systems.\n",
      "\n",
      "7. **Experimentation and Evaluation**: Learn how to evaluate your models using appropriate metrics and techniques. Understand concepts like overfitting, underfitting, bias, and variance.\n",
      "\n",
      "8. **Deployment**: Finally, learn how to deploy your models in real-world applications. This might involve working with cloud platforms, containerization, and version control.\n",
      "\n",
      "Remember, this is a complex field and it takes time to become proficient. Try to apply what you learn along the way, and don't hesitate to ask for help when you need it. Good luck!\n"
     ]
    }
   ],
   "source": [
    "print(bot.qa(\"Give me a roadmap to master machine learning.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, the ninth point is about **Ethics and Bias in Machine Learning**. As machine learning models can have a significant impact on people's lives, it's important to be aware of the ethical considerations and potential sources of bias. Here's a brief explanation of the key points:\n",
      "\n",
      "9. **Ethics and Bias in Machine Learning**:\n",
      "\n",
      "a. **Fairness**: Machine learning models should treat all individuals or groups fairly, without any discrimination based on attributes such as race, gender, age, or religion. However, it's often challenging to ensure fairness due to various forms of bias present in the data used to train the models. It's essential to identify and mitigate these biases to make the models more equitable.\n",
      "\n",
      "b. **Privacy**: Protecting sensitive information about individuals is crucial when dealing with personal data. Differential privacy is a technique that adds noise to the data to prevent the identification of specific individuals while still allowing useful insights to be obtained from the data.\n",
      "\n",
      "c. **Transparency and Interpretability**: Machine learning models can sometimes be seen as \"black boxes,\" making it difficult to understand how they arrive at their decisions. Transparent models allow users to comprehend the reasoning behind the model's predictions, which can increase trust and confidence in the system. Interpretability tools can help explain the workings of complex models.\n",
      "\n",
      "d. **Accountability and Responsibility**: Decisions made by machine learning models can significantly affect people's lives. Therefore, it's essential to establish clear accountability mechanisms for these systems. This includes identifying who is responsible for the model's outcomes, monitoring the model's performance over time, and providing ways for individuals to challenge or appeal the model's decisions.\n",
      "\n",
      "By being mindful of these ethical considerations and addressing potential sources of bias, you can build more responsible and trustworthy machine learning systems.\n"
     ]
    }
   ],
   "source": [
    "print(bot.qa(\"Explain your ninth point.\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
